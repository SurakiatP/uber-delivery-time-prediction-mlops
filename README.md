# Uber Delivery Time Prediction with MLOps
> LinkedIn Post => https://www.linkedin.com/posts/surakiat-kansa-ard-171942351_machinelearning-mlops-dataengineering-activity-7325073161790627840-SlB3?utm_source=share&utm_medium=member_desktop&rcm=ACoAAFfhzmUB4WlIO47UV21HFYwE3cHBLvtpy4A

This repository provides an end-to-end MLOps solution for predicting Uber delivery times, combining advanced data engineering, real-time inference, automated retraining, and monitoring using cutting-edge tools and best practices in the industry.

## Overview
The Uber Delivery Time Prediction system leverages machine learning and streaming data to predict delivery durations in real-time. It integrates MLOps practices, containerization, CI/CD workflows, automated retraining pipelines, and extensive monitoring and logging capabilities.

## Project Workflow

![alt text](images_readme/Project_Structure.png)

Key components include:

- Kafka: Generates and consumes real-time order data.
- FastAPI: Serves real-time predictions and model metrics.
- MLflow: Tracks experiments, manages model registry and deployment.
- Airflow: Automates daily model retraining and registration.
- DVC: Handles data versioning, ensures reproducibility.
- PostgreSQL: Stores Airflow DAG metadata, task states, execution logs.
- Prometheus + Grafana: Provides real-time analytics, performance monitoring, and visualization.

## Docker Container Stack
The entire project runs smoothly within a Docker environment to ensure reproducibility and consistency across development, staging, and production environments.

![alt text](images_readme/Docker-Ports.png)

## Project Structure

```bash
uber-delivery-time-prediction-mlops/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml                 # Docker orchestration (FastAPI, Kafka, MLflow, etc.)
‚îÇ
‚îú‚îÄ‚îÄ fastapi_app/                       # FastAPI API for serving model predictions
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements-fastapi.txt
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îÇ
‚îú‚îÄ‚îÄ producer/                          # Kafka Producer generating mock orders
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ kafka_producer.py
‚îÇ
‚îú‚îÄ‚îÄ consumer/                          # Kafka Consumer for streaming inference
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ stream_inference.py
‚îÇ
‚îú‚îÄ‚îÄ mlflow_server/                     # MLflow tracking server
‚îÇ   ‚îî‚îÄ‚îÄ mlruns/                        # Volume-mounted for persistence
‚îÇ
‚îú‚îÄ‚îÄ monitoring/                        # Monitoring stack (Prometheus + Grafana)
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îî‚îÄ‚îÄ grafana/
‚îÇ       ‚îî‚îÄ‚îÄ provisioning/
‚îÇ           ‚îú‚îÄ‚îÄ datasources/
‚îÇ           ‚îî‚îÄ‚îÄ dashboards/
‚îÇ
‚îú‚îÄ‚îÄ airflow/                           # Airflow automation for daily retraining
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ airflow_dvc_dag.py
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ data/                              # Project datasets (versioned via DVC)
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ uberdrives.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îú‚îÄ‚îÄ cleaned_uberdrives.csv
‚îÇ       ‚îú‚îÄ‚îÄ cleaned_uberdrives_with_coords.csv
‚îÇ       ‚îú‚îÄ‚îÄ uber_with_weather.csv
‚îÇ       ‚îú‚îÄ‚îÄ uber_with_weather_traffic.csv
‚îÇ       ‚îî‚îÄ‚îÄ model_ready_dataset.csv
‚îÇ
‚îú‚îÄ‚îÄ src/                               # Core pipeline logic & scripts
‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetch_orders.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetch_lat_lon.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetch_weather.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fetch_traffic.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py
‚îÇ   ‚îú‚îÄ‚îÄ modeling/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_model.py
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ       ‚îú‚îÄ‚îÄ prometheus_metrics.py
‚îÇ       ‚îî‚îÄ‚îÄ grafana_dashboard.json
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                         # Exploratory Data Analysis
‚îÇ   ‚îî‚îÄ‚îÄ data_exploration.ipynb
‚îÇ
‚îú‚îÄ‚îÄ .config/                           # Configuration files & secrets
‚îÇ   ‚îú‚îÄ‚îÄ secrets.yaml
‚îÇ   ‚îú‚îÄ‚îÄ gdrive_sa.json
‚îÇ   ‚îú‚îÄ‚îÄ params.yaml
‚îÇ   ‚îî‚îÄ‚îÄ secrets.yaml.template
‚îÇ
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ dvc.lock
‚îú‚îÄ‚îÄ dvc.yaml                           # DVC Pipeline configuration
‚îú‚îÄ‚îÄ requirements.txt                   # Global Python dependencies
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore
```

# Getting Started

## Clone Repository
```bash
git clone https://github.com/SurakiatP/uber-delivery-time-prediction-mlops.git
cd uber-delivery-time-prediction-mlops
```
## Configuration Setup (Secrets & Environment)
Before starting the project, ensure you have properly configured the following essential files with correct values:

1. `.env` File
Create a .env file at the project's root directory containing environment variables as shown below:

```bash
# Example .env configuration

# AIRFLOW CONFIG
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY= "your FERNET_KEY"
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow

# LOGGING FIX: Disable remote logging
AIRFLOW__LOGGING__REMOTE_LOGGING=False

# SECURITY FIX: Secret Key for all airflow services
AIRFLOW__WEBSERVER__SECRET_KEY=THIS_IS_A_SECRET_KEY_FOR_ALL_COMPONENTS

```

2. `config/secrets.yaml` File
This file stores sensitive information, such as API keys or credentials. Create the file at .config/secrets.yaml:

```bash
# Example secrets.yaml
weather_api_key: "YOUR_WEATHER_API_KEY"
traffic_api_key: "YOUR_TRAFFIC_API_KEY"
```
- Enter your relevant API keys for third-party services here.

>‚ö†Ô∏è Important:
Do NOT commit this file to your Git repository. It's already added to .gitignore.

>You can refer to .config/secrets.yaml.template for guidance.

3. `.config/gdrive_sa.json` File

This file contains your Google Drive Service Account credentials (JSON), enabling DVC to connect to remote storage.

Steps to create this file:

- Create a Google Cloud Service Account.
- Download the Service Account credentials JSON file.
- Save the downloaded file as .config/gdrive_sa.json.
- Sample JSON structure (example only):

```bash
{
  "type": "service_account",
  "project_id": "your_project_id",
  "private_key_id": "your_private_key_id",
  "private_key": "your_private_key",
  "client_email": "your_client_email",
  "client_id": "your_client_id"
}
```
>‚ö†Ô∏è Important:
Do NOT commit this file to your Git repository. It's already added to .gitignore.


## Docker Compose Setup
```bash
docker-compose up -d
```
Services will be accessible on their respective ports:

- FastAPI: http://localhost:8000
- Airflow: http://localhost:8080
- MLflow: http://localhost:5000
- Grafana: http://localhost:3000
- Prometheus: http://localhost:9090

## Running Experiments & Retraining
- Manual Retraining: Triggered via Airflow UI (daily_dvc_retrain DAG).
- Experiment Tracking: Monitor experiments through MLflow UI.

## Monitoring & Observability
Real-time model metrics and predictions are monitored using Prometheus and visualized via Grafana dashboards.

## Technologies
- Kafka for streaming data.
- FastAPI for model deployment.
- MLflow for experiment tracking and model registry.
- Apache Airflow for pipeline orchestration.
- DVC for data versioning.
- Docker for containerization.
- Prometheus & Grafana for monitoring and analytics.
- PostgreSQL as Airflow's backend database.

# Contribution
Contributions are welcome! Please open issues or submit pull requests.

---

## Project Author

| Name           | Contact Information                                                  |
|----------------|----------------------------------------------------------------------|
| **Surakiat P.** |                                                                      |
| üìß Email       | [surakiat.0723@gmail.com](mailto:surakiat.0723@gmail.com)   |
| üîó LinkedIn    | [linkedin.com/in/surakiat](https://www.linkedin.com/in/surakiat-kansa-ard-171942351/)     |
| üåê GitHub      | [github.com/SurakiatP](https://github.com/SurakiatP)                 |
